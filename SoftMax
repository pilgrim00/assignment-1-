
import math
import numpy as np
import torch
class SoftMax():
    def __init__(self):
        self.w = (np.zeros((784, 10))+1)
        self.b = (np.zeros((1, 10))+1)[0]
    def loss1(self,score,y):
        a=0
        for i in range(len(score)):
            a+=self.crossentropy(y,score)
        return a/len(score)
    def loss2(self,a,b):
        loss=0
        for i in range(10):
            loss+=0.5*(a[i]-b[i])**2
        return loss
    def lossbackward(self):
        for i in range(10):
            y1 = self.y[i]
            score1 = self.score[i]
            for j in range(784):
                d1=score1-y1
                dw=self.x[i]*d1
                self.w[j][i]=self.w[j][i]-dw
            self.b[i]=self.b[i]-d1
    def predict(self,x,y):
        x=x.reshape(1,784)
        x=x[0]
        self.x=x
        self.score=x.dot(self.w)+self.b
        out = torch.tensor(self.score)
        self.score=torch.sigmoid(out)
        self.y=self.onehot(y)
        self.lossbackward()
        return self.score
    def onehot(self,y):
        nums=[]
        for i in range(10):
            if y==i:
              nums.append(1)
            if y!=i:
              nums.append(0)
        return nums
    def softmax(self,x:list):
        b=0
        for i in range(10):
            '''f x[i]<=-1000:
                x[i]=1
                b+=1
            if x[i]>=1000:
                x[i]=1000
                b+=1000
            else:'''
            x[i]=np.exp(x[i])
            b+=x[i]
        for j in range(10):
            x[j]=x[j]/b
        return x
